{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde465d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0757cb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0370a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import glob\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    " # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path\n",
    "\n",
    "PATH = \"three_label_classifier.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f4f01c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37] [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18] [19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37]\n"
     ]
    }
   ],
   "source": [
    "# Training 25672 Total - 13000 Healthy / 11000 Has_DKD / 1,700 CKD_noDM\n",
    "# Validation 18071 Total - 15,988 Healthy / 1879 Has_DKD / 204 CKD_noDM\n",
    "# Testing 14861 Total - 11,470 Healthy / 2318 Has_DKD / 1,073 CKD_noDM\n",
    "\n",
    "# Try to find an equal number of patches from each set do better sampling\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(512),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "temp_DKD = glob.glob('/data1/DKDimages/Three_Label_WSI/KPMP_WSI/Has_DKD/*')\n",
    "wsi_HasDKD = []\n",
    "wsi_all = []\n",
    "for x in temp_DKD:\n",
    "    \n",
    "    dkd_split = x.split(\"/\")[-1].split(\".\")[0].split('_')[0]\n",
    "#     print(dkd_split)\n",
    "    wsi_HasDKD.append(dkd_split)\n",
    "    wsi_all.append(dkd_split)\n",
    "# print(\"has DKD\",wsi_HasDKD)\n",
    "# print(len(wsi_HasDKD))\n",
    "temp_Healthy = glob.glob('/data1/DKDimages/Three_Label_WSI/KPMP_WSI/Healthy/*')\n",
    "wsi_Healthy = []\n",
    "for x in temp_Healthy:\n",
    "    \n",
    "    dkd_split = x.split(\"/\")[-1].split(\".\")[0].split('_')[0]\n",
    "#     print(dkd_split)\n",
    "    wsi_Healthy.append(dkd_split)\n",
    "    wsi_all.append(dkd_split)\n",
    "# print(\"Healthy\",wsi_Healthy)\n",
    "# print(len(wsi_Healthy))\n",
    "# print(wsi_all)\n",
    "# print(\"all len\",len(wsi_all))\n",
    "\n",
    "temp_CKD_noDM = glob.glob('/data1/DKDimages/Three_Label_WSI/KPMP_WSI/CKD_noDM/*')\n",
    "wsi_CKD_noDM = []\n",
    "for x in temp_CKD_noDM:\n",
    "    \n",
    "    dkd_split = x.split(\"/\")[-1].split(\".\")[0].split('_')[0]\n",
    "#     print(dkd_split)\n",
    "    wsi_CKD_noDM.append(dkd_split)\n",
    "    wsi_all.append(dkd_split)\n",
    "# print(\"ckd\",wsi_CKD_noDM)\n",
    "# print(len(wsi_CKD_noDM))\n",
    "\n",
    "\n",
    "# print(wsi_all)\n",
    "# print(\"all len\",len(wsi_all))\n",
    "wsi_dict = dict()\n",
    "for i in wsi_all:\n",
    "    wsi_dict[i] = dict()\n",
    "    wsi_dict[i][\"Healthy\"] = 0\n",
    "    wsi_dict[i][\"Has_DKD\"] = 0\n",
    "    wsi_dict[i][\"CKD_noDM\"] = 0\n",
    "# print(wsi_dict)\n",
    "\n",
    "\n",
    "params = {'batch_size': 4,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 8}\n",
    "\n",
    "class_names = ('CKD_noDM','Has_DKD', 'Healthy', )\n",
    "# data_dir = '/projectnb/vkolagrp/Tsurendr/Deepslide-NewLabels/train_folder/'\n",
    "# print(os.path.exists(data_dir))\n",
    "\n",
    "# training_set = Dataset((os.path.join(data_dir, 'train')), labels)\n",
    "# training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "\n",
    "# validation_set = Dataset(os.path.join(data_dir, 'val'), labels)\n",
    "# validation_generator = torch.utils.data.DataLoader(validation_set, **params)\n",
    "\n",
    "\n",
    "\n",
    "# image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "#                                           data_transforms[x])\n",
    "#                   for x in ['train', 'val']}\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kf = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "for train_index, test_index in kf.split('/data1/DKDimages/All_patch_Folders/*/*'):\n",
    "    print(train_index,test_index)\n",
    "\n",
    "# image_datasets_train = ImageFolderWithPaths('/data1/DKDimages/Pyhist_Patches/train', transform = data_transforms)\n",
    "# print(len(image_datasets_train))\n",
    "# image_datasets_val = ImageFolderWithPaths('/data1/DKDimages/Pyhist_Patches/val', transform = data_transforms)\n",
    "# print(len(image_datasets_val))\n",
    "# image_datasets_test = ImageFolderWithPaths('/data1/DKDimages/Pyhist_Patches/test', transform = data_transforms)\n",
    "# print(len(image_datasets_test))\n",
    "# # print(image_datasets_val)\n",
    "# print(\"validation set length: \", len(image_datasets_val))\n",
    "# dataloader_train = torch.utils.data.DataLoader(image_datasets_train, batch_size=4,\n",
    "#                                              shuffle=True, num_workers=8)\n",
    "# dataloader_val = torch.utils.data.DataLoader(image_datasets_val, batch_size=4,\n",
    "#                                              shuffle=True, num_workers=8)\n",
    "# dataloader_test = torch.utils.data.DataLoader(image_datasets_test, batch_size=4,\n",
    "#                                              shuffle=False, num_workers=8)\n",
    "# # dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "# #                                              shuffle=True, num_workers=8)\n",
    "# #                for x in ['train', 'val']}\n",
    "# print(\"Dataloader size: \", len(dataloader_val))\n",
    "# dataset_sizes_train = len(image_datasets_train)\n",
    "# dataset_sizes_val = len(image_datasets_val)\n",
    "# # class_names = image_datasets['train'].classes\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# path_sum = 0\n",
    "# # for i, data in enumerate(dataloader_val):\n",
    "# #     images,labels,paths = data\n",
    "# # #     print( paths[3])\n",
    "# #     if paths[0]: \n",
    "# #         path_sum+=1\n",
    "# #         print(\"path 0\", paths[0])\n",
    "# #     if paths[1]:\n",
    "# #         path_sum+=1\n",
    "# #         print(\"path 1\", paths[1])\n",
    "        \n",
    "# #     try:\n",
    "# #         if paths[2]:\n",
    "# #             path_sum+=1\n",
    "# #             print(\"path 2\", paths[2])\n",
    "            \n",
    "# #     except IndexError:\n",
    "# #         pass\n",
    "    \n",
    "# #     try:\n",
    "# #         if paths[3]:\n",
    "# #             path_sum+=1\n",
    "# #             print(\"path 3\", paths[3])\n",
    "            \n",
    "# #     except IndexError:\n",
    "# #         pass\n",
    "# # print(\"done\")\n",
    "# # print(path_sum)\n",
    "# def imshow(inp, title=None):\n",
    "#     \"\"\"Imshow for Tensor.\"\"\"\n",
    "#     inp = inp.numpy().transpose((1, 2, 0))\n",
    "#     mean = np.array([0.485, 0.456, 0.406])\n",
    "#     std = np.array([0.229, 0.224, 0.225])\n",
    "#     inp = std * inp + mean\n",
    "#     inp = np.clip(inp, 0, 1)\n",
    "#     plt.imshow(inp)\n",
    "#     if title is not None:\n",
    "#         plt.title(title)\n",
    "#     plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# # # Get a batch of training data\n",
    "# inputs, classes, paths = next(iter(dataloader_train))\n",
    "# # print(classes)\n",
    "# # print(inputs)\n",
    "# # Make a grid from batch\n",
    "# out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# imshow(out, title=[class_names[x] for x in classes])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8d98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
    "    since = time.time()\n",
    "#     print(\"hello\")\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            if phase == \"train\":\n",
    "                \n",
    "                for inputs, labels, paths in dataloader_train:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "#                     print(min(labels))\n",
    "#                     print(max(labels))\n",
    "#                     print(inputs.shape)\n",
    "        \n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "#                         print(torch.max(outputs, 1))\n",
    "#                         print(torch.min(outputs, 1))\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            \n",
    "            if phase == \"val\":\n",
    "                \n",
    "                for inputs, labels, paths in dataloader_val:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            if phase == 'train':\n",
    "                epoch_loss = running_loss / dataset_sizes_train\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes_train\n",
    "            if phase == 'val':\n",
    "                epoch_loss = running_loss / dataset_sizes_val\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes_val\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model, PATH)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#Whole Covnet        \n",
    "# model_ft = models.resnet18(pretrained=True)\n",
    "# num_ftrs = model_ft.fc.in_features\n",
    "# # Here the size of each output sample is set to 2.\n",
    "# # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "# model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# model_ft = model_ft.to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # Observe that all parameters are being optimized\n",
    "# optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "# model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "#                        num_epochs=25)\n",
    "# visualize_model(model_ft)\n",
    "\n",
    "#final layer ########################\n",
    "\n",
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "model_conv.outlayer = nn.Linear(512 * 1 * 1, 3)\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "\n",
    "\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bba68b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def visualize_model(model, num_images,dataloader):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "    path_sum = 0\n",
    "    len_class_names = len(class_names)\n",
    "    confusion_matrix = torch.zeros(len_class_names, len_class_names)\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            inputs, labels, filenames = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            print(labels) \n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "#             print(filenames)\n",
    "            try:\n",
    "                if filenames[0]: \n",
    "                    path_sum+=1\n",
    "#                     print(\"path 0\", filenames[0])\n",
    "#                     print(\"prediction: \", class_names[preds[0]])\n",
    "                    temp = filenames[0].split(\"/\")[-1].split(\".\")[0].split('_')[0]\n",
    "    #                 dict_key = temp[0]+\"_\"+temp[1]+\"_\"+temp[2]+\"_\"+temp[3]\n",
    "                    dict_key = temp\n",
    "    #                 print(dict_key)\n",
    "                    if dict_key in wsi_all:\n",
    "                        if class_names[preds[0]] == \"Has_DKD\":\n",
    "                            wsi_dict[dict_key][\"Has_DKD\"] += 1\n",
    "                        elif class_names[preds[0]] == \"Healthy\":\n",
    "                            wsi_dict[dict_key][\"Healthy\"] += 1\n",
    "                        elif class_names[preds[0]] == \"CKD_noDM\":\n",
    "                            wsi_dict[dict_key][\"CKD_noDM\"] += 1\n",
    "            except IndexError:\n",
    "                pass\n",
    "            try:\n",
    "                \n",
    "                if filenames[1]:\n",
    "                    path_sum+=1\n",
    "#                     print(\"path 1\", filenames[1])\n",
    "#                     print(\"prediction: \", class_names[preds[1]])\n",
    "                    temp = filenames[1].split(\"/\")[-1].split(\".\")[0].split('_')[0]\n",
    "    #                 dict_key = temp[0]+\"_\"+temp[1]+\"_\"+temp[2]+\"_\"+temp[3]\n",
    "                    dict_key = temp\n",
    "    #                 print(dict_key)                \n",
    "                    if dict_key in wsi_all:\n",
    "                        if class_names[preds[1]] == \"Has_DKD\":\n",
    "                            wsi_dict[dict_key][\"Has_DKD\"] += 1\n",
    "                        elif class_names[preds[1]] == \"Healthy\":\n",
    "                            wsi_dict[dict_key][\"Healthy\"] += 1\n",
    "                        elif class_names[preds[1]] == \"CKD_noDM\":\n",
    "                            wsi_dict[dict_key][\"CKD_noDM\"] += 1\n",
    "    #             print(filenames[2])\n",
    "            except IndexError:\n",
    "                pass\n",
    "            try:\n",
    "                if filenames[2]:\n",
    "                    path_sum+=1\n",
    "#                     print(\"path 2\", filenames[2])\n",
    "#                     print(\"prediction: \", class_names[preds[2]])\n",
    "                    temp = filenames[2].split(\"/\")[-1].split(\".\")[0].split('_')[0]\n",
    "                    dict_key = temp\n",
    "#                     dict_key = temp[0]+\"_\"+temp[1]+\"_\"+temp[2]+\"_\"+temp[3]\n",
    "#                     print(dict_key)                    \n",
    "                    if dict_key in wsi_all:\n",
    "                        if class_names[preds[2]] == \"Has_DKD\":\n",
    "                            wsi_dict[dict_key][\"Has_DKD\"] += 1\n",
    "                        elif class_names[preds[2]] == \"Healthy\":\n",
    "                            wsi_dict[dict_key][\"Healthy\"] += 1\n",
    "                        elif class_names[preds[2]] == \"CKD_noDM\":\n",
    "                            wsi_dict[dict_key][\"CKD_noDM\"] += 1\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                if filenames[3]:\n",
    "                    path_sum+=1\n",
    "#                     print(\"path 3\", filenames[3])\n",
    "#                     print(\"prediction: \", class_names[preds[3]])\n",
    "                    temp = filenames[3].split(\"/\")[-1].split(\".\")[0].split('_')[0]\n",
    "                    dict_key = temp\n",
    "#                     dict_key = temp[0]+\"_\"+temp[1]+\"_\"+temp[2]+\"_\"+temp[3]\n",
    "#                     print(dict_key)\n",
    "                    if dict_key in wsi_all:\n",
    "                        if class_names[preds[3]] == \"Has_DKD\":\n",
    "                            wsi_dict[dict_key][\"Has_DKD\"] += 1\n",
    "                        elif class_names[preds[3]] == \"Healthy\":\n",
    "                            wsi_dict[dict_key][\"Healthy\"] += 1\n",
    "                        elif class_names[preds[3]] == \"CKD_noDM\":\n",
    "                            wsi_dict[dict_key][\"CKD_noDM\"] += 1\n",
    "            except IndexError:\n",
    "                pass\n",
    "        \n",
    "            for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "                            confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "#             print(filenames[0])\n",
    "#             [print(class_names[x]) for x in preds]\n",
    "        \n",
    "#             for j in range(inputs.size()[0]):\n",
    "#                 images_so_far += 1\n",
    "#                 ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "#                 ax.axis('off')\n",
    "#                 ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "#                 imshow(inputs.cpu().data[j])\n",
    "\n",
    "#                 if images_so_far == num_images:\n",
    "#                     model.train(mode=was_training)\n",
    "#                     return\n",
    "#         print(confusion_matrix)\n",
    "#         print(confusion_matrix.diag()/confusion_matrix.sum(1))\n",
    "\n",
    "#         sns.heatmap(confusion_matrix, annot=True)\n",
    "\n",
    "\n",
    "        ax= plt.subplot()\n",
    "        sns.heatmap(confusion_matrix, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "        \n",
    "        # labels, title and ticks\n",
    "        ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "        ax.set_title('Confusion Matrix'); \n",
    "        ax.xaxis.set_ticklabels(['CKD_noDM', 'Has_DKD','Healthy']); ax.yaxis.set_ticklabels(['CKD_noDM', 'Has_DKD','Healthy']);\n",
    "        print('Predicted label accuracy',(confusion_matrix.diag()/confusion_matrix.sum(1)))\n",
    "        model.train(mode=was_training)\n",
    "\n",
    "model = torch.load(PATH)\n",
    "model.eval()\n",
    "visualize_model(model,6,dataloader_val)\n",
    "print(\"Validation Slide Status: \")\n",
    "# print(wsi_all)\n",
    "for k in wsi_all:\n",
    "#     print(k)\n",
    "#     print(\"WSI: \", k, \" healthy: \", wsi_dict[k]['Healthy'], \"has DKD: \", wsi_dict[k]['Has_DKD'],\"CKD_noDM\" ,wsi_dict[k]['CKD_noDM'])\n",
    "    if (max(wsi_dict[k]['Healthy'], wsi_dict[k]['Has_DKD'],wsi_dict[k]['CKD_noDM']) == wsi_dict[k]['Healthy']) and (wsi_dict[k]['Healthy'] != 0 or wsi_dict[k]['Has_DKD'] != 0 or wsi_dict[k]['CKD_noDM'] != 0 ):\n",
    "        print(\"WSI: \", k, \"Status: Healthy\" )\n",
    "    elif max(wsi_dict[k]['Healthy'], wsi_dict[k]['Has_DKD'],wsi_dict[k]['CKD_noDM']) == wsi_dict[k]['Has_DKD'] and (wsi_dict[k]['Healthy'] != 0 or wsi_dict[k]['Has_DKD'] != 0 or wsi_dict[k]['CKD_noDM'] != 0 ):\n",
    "        print(\"WSI: \", k, \"Status: Has_DKD\" )\n",
    "    elif max(wsi_dict[k]['Healthy'], wsi_dict[k]['Has_DKD'],wsi_dict[k]['CKD_noDM']) == wsi_dict[k]['CKD_noDM'] and (wsi_dict[k]['Healthy'] != 0 or wsi_dict[k]['Has_DKD'] != 0 or wsi_dict[k]['CKD_noDM'] != 0 ):\n",
    "        print(\"WSI: \", k, \"Status: CKD_noDM\" )\n",
    "plt.ioff()\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00fb4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataiter = iter(dataloader_test)\n",
    "# images, labels, paths = dataiter.next()\n",
    "# images = images.cuda()\n",
    "# labels = labels.cuda()\n",
    "# for k in range(4): \n",
    "#     imshow(images.cpu().data[k]) \n",
    "# print('GroundTruth: ', ' '.join('%5s' % class_names[labels[j]] for j in range(4)))\n",
    "\n",
    "# outputs = model_conv(images)\n",
    "\n",
    "# _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# print('Predicted: ', ' '.join('%5s' % class_names[predicted[j]]\n",
    "#                               for j in range(4)))\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "# with torch.no_grad():\n",
    "#     path_sum = 0\n",
    "#     for i, data in enumerate(dataloader_test):\n",
    "#         inputs, labels, filenames = data\n",
    "#         images = images.cuda() \n",
    "#         labels = labels.cuda()\n",
    "   \n",
    "#         # calculate outputs by running images through the network\n",
    "#         outputs = model_conv(images)\n",
    "#         # the class with the highest energy is what we choose as prediction\n",
    "#         _, preds = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "\n",
    "        \n",
    "#         try:\n",
    "#             if filenames[0]:\n",
    "#                 path_sum+=1\n",
    "#                 print(\"path 1\", filenames[0])\n",
    "#                 print(\"prediction: \", class_names[preds[0]])\n",
    "#                 temp = filenames[1].split(\"/\")[-1].split(\".\")[0].split('_')[0]\n",
    "#     #                 dict_key = temp[0]+\"_\"+temp[1]+\"_\"+temp[2]+\"_\"+temp[3]\n",
    "#                 dict_key = temp\n",
    "#     #             print(dict_key)                \n",
    "#                 if dict_key in wsi_all:\n",
    "#                     if class_names[preds[0]] == \"Has_DKD\":\n",
    "#                         wsi_dict[dict_key][\"Has_DKD\"] += 1\n",
    "#                     elif class_names[preds[0]] == \"Healthy\":\n",
    "#                         wsi_dict[dict_key][\"Healthy\"] += 1\n",
    "#                     elif class_names[preds[0]] == \"CKD_noDM\":\n",
    "#                         wsi_dict[dict_key][\"CKD_noDM\"] += 1   \n",
    "#     #             print(filenames[2])\n",
    "#           except IndexError:\n",
    "#             pass\n",
    "#         try:\n",
    "#             if filenames[1]:\n",
    "#                 path_sum+=1\n",
    "#                 print(\"path 1\", filenames[1])\n",
    "#                 print(\"prediction: \", class_names[preds[1]])\n",
    "#                 temp = filenames[1].split(\"/\")[-1].split(\".\")[0].split('_')[0]\n",
    "#     #                 dict_key = temp[0]+\"_\"+temp[1]+\"_\"+temp[2]+\"_\"+temp[3]\n",
    "#                 dict_key = temp\n",
    "#     #             print(dict_key)                \n",
    "#                 if dict_key in wsi_all:\n",
    "#                     if class_names[preds[1]] == \"Has_DKD\":\n",
    "#                         wsi_dict[dict_key][\"Has_DKD\"] += 1\n",
    "#                     elif class_names[preds[1]] == \"Healthy\":\n",
    "#                         wsi_dict[dict_key][\"Healthy\"] += 1\n",
    "#                     elif class_names[preds[1]] == \"CKD_noDM\":\n",
    "#                         wsi_dict[dict_key][\"CKD_noDM\"] += 1   \n",
    "#     #             print(filenames[2])\n",
    "#           except IndexError:\n",
    "#             pass\n",
    "#         try:\n",
    "#             if filenames[2]:\n",
    "#                 path_sum+=1\n",
    "#                 print(\"path 2\", filenames[2])\n",
    "#                 print(\"prediction: \", class_names[preds[2]])\n",
    "#                 temp = filenames[2].split(\"/\")[-1].split(\".\")[0].split('_')[0]\n",
    "#                 dict_key = temp\n",
    "# #                     dict_key = temp[0]+\"_\"+temp[1]+\"_\"+temp[2]+\"_\"+temp[3]\n",
    "# #                 print(dict_key)                    \n",
    "#                 if dict_key in wsi_all:\n",
    "#                     if class_names[preds[2]] == \"Has_DKD\":\n",
    "#                         wsi_dict[dict_key][\"Has_DKD\"] += 1\n",
    "#                     elif class_names[preds[2]] == \"Healthy\":\n",
    "#                         wsi_dict[dict_key][\"Healthy\"] += 1\n",
    "#                     elif class_names[preds[2]] == \"CKD_noDM\":\n",
    "#                         wsi_dict[dict_key][\"CKD_noDM\"] += 1   \n",
    "#         except IndexError:\n",
    "#             pass\n",
    "\n",
    "#         try:\n",
    "#             if filenames[3]:\n",
    "#                 path_sum+=1\n",
    "#                 print(\"path 3\", filenames[3])\n",
    "#                 print(\"prediction: \", class_names[preds[3]])\n",
    "#                 temp = filenames[3].split(\"/\")[-1].split(\".\")[0].split('_')[0]\n",
    "#                 dict_key = temp\n",
    "# #                     dict_key = temp[0]+\"_\"+temp[1]+\"_\"+temp[2]+\"_\"+temp[3]\n",
    "# #                 print(dict_key)\n",
    "#                 if dict_key in wsi_all:\n",
    "#                     if class_names[preds[3]] == \"Has_DKD\":\n",
    "#                         wsi_dict[dict_key][\"Has_DKD\"] += 1\n",
    "#                     elif class_names[preds[3]] == \"Healthy\":\n",
    "#                         wsi_dict[dict_key][\"Healthy\"] += 1\n",
    "#                     elif class_names[preds[3]] == \"CKD_noDM\":\n",
    "#                         wsi_dict[dict_key][\"CKD_noDM\"] += 1   \n",
    "#         except IndexError:\n",
    "#             pass\n",
    "\n",
    "# # print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "# #     100 * correct / total))\n",
    "\n",
    "\n",
    "# # prepare to count predictions for each class\n",
    "# correct_pred = {classname: 0 for classname in class_names}\n",
    "# total_pred = {classname: 0 for classname in class_names}\n",
    "\n",
    "# # again no gradients needed\n",
    "# with torch.no_grad():\n",
    "#     for i,data in enumerate(dataloader_test):\n",
    "#         images, labels, paths = data\n",
    "#         images = images.cuda()\n",
    "#         labels = labels.cuda()\n",
    "#         outputs = model_conv(images)\n",
    "#         _, predictions = torch.max(outputs, 1)\n",
    "#         # collect the correct predictions for each class\n",
    "#         for label, prediction in zip(labels, predictions):\n",
    "#             if label == prediction:\n",
    "#                 correct_pred[class_names[label]] += 1\n",
    "#             total_pred[class_names[label]] += 1\n",
    "\n",
    "\n",
    "# # # print accuracy for each class\n",
    "# for classname, correct_count in correct_pred.items():\n",
    "#     accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "#     print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "#                                                    accuracy))\n",
    "\n",
    "visualize_model(model,6,dataloader_test)    \n",
    "print(\"Testing & Validation Slide Status: \")\n",
    "# print(wsi_all)\n",
    "for k in wsi_all:\n",
    "#     print(k)\n",
    "#     print(\"WSI: \", k, \" healthy: \", wsi_dict[k]['Healthy'], \"has DKD: \", wsi_dict[k]['Has_DKD'],\"CKD_noDM\" ,wsi_dict[k]['CKD_noDM'])\n",
    "    if (max(wsi_dict[k]['Healthy'], wsi_dict[k]['Has_DKD'],wsi_dict[k]['CKD_noDM']) == wsi_dict[k]['Healthy']) and (wsi_dict[k]['Healthy'] != 0 or wsi_dict[k]['Has_DKD'] != 0 or wsi_dict[k]['CKD_noDM'] != 0 ):\n",
    "        print(\"WSI: \", k, \"Status: Healthy\" )\n",
    "    elif max(wsi_dict[k]['Healthy'], wsi_dict[k]['Has_DKD'],wsi_dict[k]['CKD_noDM']) == wsi_dict[k]['Has_DKD'] and (wsi_dict[k]['Healthy'] != 0 or wsi_dict[k]['Has_DKD'] != 0 or wsi_dict[k]['CKD_noDM'] != 0 ):\n",
    "        print(\"WSI: \", k, \"Status: Has_DKD\" )\n",
    "    elif max(wsi_dict[k]['Healthy'], wsi_dict[k]['Has_DKD'],wsi_dict[k]['CKD_noDM']) == wsi_dict[k]['CKD_noDM'] and (wsi_dict[k]['Healthy'] != 0 or wsi_dict[k]['Has_DKD'] != 0 or wsi_dict[k]['CKD_noDM'] != 0 ):\n",
    "        print(\"WSI: \", k, \"Status: CKD_noDM\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f292ca44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filenames</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>502d2911-4815-44e0-b66f-b1910e8808e9_S-1910-00...</td>\n",
       "      <td>H-CKD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dbdcd9a2-62a7-4988-86a2-90cf1ea57242_S-1910-00...</td>\n",
       "      <td>H-CKD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5d07f47b-dd32-4c09-92ea-fd084f4ca2e8_18-162_PA...</td>\n",
       "      <td>HEALTHY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>829067a5-eda4-415d-8e2e-fa9a15ac583c_18-162_PA...</td>\n",
       "      <td>HEALTHY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61a1c11d-d869-4868-ba9b-5f31fbd1dbff_18-162_PA...</td>\n",
       "      <td>HEALTHY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7daf471c-957e-4ebf-a206-bc0fef11cf2f_18-162_PA...</td>\n",
       "      <td>HEALTHY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>760649cb-be20-4559-8f29-5fb4a3225e43_18-162_PA...</td>\n",
       "      <td>HEALTHY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16dc8e6f-524c-48cb-9252-46140fd74090_18-162_PA...</td>\n",
       "      <td>HEALTHY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2fda166d-a6d0-46a7-acfa-124e0e51dce2_S-2001-01...</td>\n",
       "      <td>DKD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>060e75ad-0819-4007-b5bc-18a39dbcefdd_S-2001-01...</td>\n",
       "      <td>DKD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>59eaa15d-a1d5-4990-93bc-7b53b590fab5_S-2006-00...</td>\n",
       "      <td>DKD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>f1f7bccc-feb9-42fe-a76c-8a488e6b1986_S-2006-00...</td>\n",
       "      <td>DKD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>a71eff4a-4751-423f-91aa-c273ea6c08cf_S-2001-00...</td>\n",
       "      <td>DKD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>e6c166fd-0b8b-4dbe-8fea-7fd3310d9243_S-2001-00...</td>\n",
       "      <td>DKD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filenames   labels\n",
       "0   502d2911-4815-44e0-b66f-b1910e8808e9_S-1910-00...    H-CKD\n",
       "1   dbdcd9a2-62a7-4988-86a2-90cf1ea57242_S-1910-00...    H-CKD\n",
       "2   5d07f47b-dd32-4c09-92ea-fd084f4ca2e8_18-162_PA...  HEALTHY\n",
       "3   829067a5-eda4-415d-8e2e-fa9a15ac583c_18-162_PA...  HEALTHY\n",
       "4   61a1c11d-d869-4868-ba9b-5f31fbd1dbff_18-162_PA...  HEALTHY\n",
       "5   7daf471c-957e-4ebf-a206-bc0fef11cf2f_18-162_PA...  HEALTHY\n",
       "6   760649cb-be20-4559-8f29-5fb4a3225e43_18-162_PA...  HEALTHY\n",
       "7   16dc8e6f-524c-48cb-9252-46140fd74090_18-162_PA...  HEALTHY\n",
       "8   2fda166d-a6d0-46a7-acfa-124e0e51dce2_S-2001-01...      DKD\n",
       "9   060e75ad-0819-4007-b5bc-18a39dbcefdd_S-2001-01...      DKD\n",
       "10  59eaa15d-a1d5-4990-93bc-7b53b590fab5_S-2006-00...      DKD\n",
       "11  f1f7bccc-feb9-42fe-a76c-8a488e6b1986_S-2006-00...      DKD\n",
       "12  a71eff4a-4751-423f-91aa-c273ea6c08cf_S-2001-00...      DKD\n",
       "13  e6c166fd-0b8b-4dbe-8fea-7fd3310d9243_S-2001-00...      DKD"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openslide import OpenSlide\n",
    "df_labels = pd.read_csv(\"Test_labels.csv\")\n",
    "df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49477260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class MyDaatset(Dataset):\n",
    "    def __init__(self, path, labels_wsi,transform):\n",
    "        self.files = glob.glob(path)\n",
    "        print(type(self.files))\n",
    "        self.transform = transform\n",
    "        self.labels = labels_wsi\n",
    "    def __getitem__(self, item):\n",
    "        file = self.files[item]\n",
    "        name = file\n",
    "        label = self.labels\n",
    "        file = Image.open(file)\n",
    "        file = self.transform(file)\n",
    "        if label.startswith(\"H-CKD\"):\n",
    "            label=0\n",
    "        elif label.startswith(\"Healthy\"):\n",
    "            label=2\n",
    "        elif label.startswith(\"Has_DKD\"):\n",
    "            label=1\n",
    "        return file,label,name\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "\n",
    "print(\"hei\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5c65b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502d2911-4815-44e0-b66f-b1910e8808e9_S-1910-000043_PAS_1of2\n",
      "H-CKD\n",
      "/data1/DKDimages/ALL_WSI/502d2911-4815-44e0-b66f-b1910e8808e9_S-1910-000043_PAS_1of2.svs\n",
      "True\n",
      "(139509, 53875)\n",
      "(2179, 841)\n",
      "True\n",
      "/data1/DKDimages/All_patch_Folders/502d2911-4815-44e0-b66f-b1910e8808e9_S-1910-000043_PAS_1of2_tiles\n",
      "True\n",
      "<class 'list'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "integer argument expected, got float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10861/515704299.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m \u001b[0mheatmap_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwsi_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_10861/515704299.py\u001b[0m in \u001b[0;36mheatmap_generation\u001b[0;34m(model, wsi_name, label)\u001b[0m\n\u001b[1;32m     78\u001b[0m                         \u001b[0;31m# heat = np.full(shape=(512, 512,3),fill_value=(0,0,255)).astype(np.uint32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                         \u001b[0;31m# heat = Image.fromarray((heat * 255).astype(np.uint8))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                         \u001b[0mheat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSCALE_FACTOR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSCALE_FACTOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Healthy\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                         \u001b[0;31m# heat = np.full(shape=(512, 512,3),fill_value=(0,255,0)).astype(np.uint32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/dpd/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mnew\u001b[0;34m(mode, size, color)\u001b[0m\n\u001b[1;32m   2703\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImagePalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImagePalette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2704\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2705\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: integer argument expected, got float"
     ]
    }
   ],
   "source": [
    "#so i pass a wsi into the function\n",
    "# inside the function i get the size of the wsi as well as the tsv and patches for it\n",
    "# so the data loader essentially tests only the values of 1 image at a time.\n",
    "from PIL import Image, ImageOps, ImageEnhance \n",
    "import openslide\n",
    "import math\n",
    "def slide_to_scaled_pil_image(slide, SCALE_FACTOR):\n",
    "\n",
    "    large_w, large_h = slide.dimensions\n",
    "    new_w = math.floor(large_w / SCALE_FACTOR)\n",
    "    new_h = math.floor(large_h / SCALE_FACTOR)\n",
    "    level = slide.get_best_level_for_downsample(SCALE_FACTOR)\n",
    "    whole_slide_image = slide.read_region((0, 0), level, slide.level_dimensions[level])\n",
    "    whole_slide_image = whole_slide_image.convert(\"RGB\")\n",
    "    img = whole_slide_image.resize((new_w, new_h), Image.BILINEAR)\n",
    "    return img, large_w, large_h, new_w, new_h\n",
    "\n",
    "\n",
    "\n",
    "def heatmap_generation(model,wsi_name,label):\n",
    "    SCALE_FACTOR=64\n",
    "    wsi_path = '/data1/DKDimages/ALL_WSI/'+wsi_name+'.svs'\n",
    "    print(wsi_path)\n",
    "    print(os.path.exists(wsi_path))\n",
    "    wsi_image = OpenSlide(wsi_path)\n",
    "    # pilImage = Image.open(wsi_path)\n",
    "    img = openslide.OpenSlide(wsi_path)\n",
    "    ds_img, large_w, large_h, new_w, new_h = slide_to_scaled_pil_image(img, SCALE_FACTOR)\n",
    "    ds_img.save(\"test.png\")\n",
    "    # image = openslide.open_slide(wsi_path)\n",
    "    # new_IM = Image.convert(image)\n",
    "    # print(image.dimensions)\n",
    "    path_sum = 0\n",
    "    \n",
    "    print(wsi_image.dimensions)\n",
    "    print(ds_img.size)\n",
    "    target_size = ds_img.size\n",
    "    heat_map = Image.new('RGB', target_size)\n",
    "    tsv_loc = '/data1/DKDimages/Pyhist_WSI_Image_Coords/' + wsi_name+ '/tile_selection.tsv'\n",
    "    print(os.path.exists(tsv_loc))\n",
    "    patch_loc = '/data1/DKDimages/All_patch_Folders/' + wsi_name+'_tiles'\n",
    "    print(patch_loc)\n",
    "    print(os.path.exists(patch_loc))\n",
    "    tsv_coords = pd.read_csv(tsv_loc,sep='\\t')\n",
    "    # print(tsv_coords)\n",
    "    # print(glob.glob(patch_loc+'/*'))\n",
    "    temp = MyDaatset('/data1/DKDimages/All_patch_Folders/502d2911-4815-44e0-b66f-b1910e8808e9_S-1910-000043_PAS_1of2_tiles/*',labels_wsi = label,transform=data_transforms)\n",
    "    # print(len(temp))\n",
    "    # image_datasets_test_new = tempFolder('/data1/DKDimages/All_patch_Folders/502d2911-4815-44e0-b66f-b1910e8808e9_S-1910-000043_PAS_1of2_tiles',labels = label,transform = data_transforms)\n",
    "    dataloader_test_new = torch.utils.data.DataLoader(temp, batch_size=4,\n",
    "                                             shuffle=False, num_workers=8)  \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader_test_new):\n",
    "            inputs, labels, filenames = data\n",
    "            # print(inputs,labels,filenames)\n",
    "            # print(type(labels))\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "              \n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "            # print(preds)\n",
    "            try:\n",
    "                if filenames[0]: \n",
    "                    path_sum+=1\n",
    "                    # print(\"path 0\", filenames[0])\n",
    "                    # print(\"prediction: \", class_names[preds[0]])\n",
    "                    temp = filenames[0].split(\"/\")[-1].split(\".\")[0]\n",
    "    #                 dict_key = temp[0]+\"_\"+temp[1]+\"_\"+temp[2]+\"_\"+temp[3]\n",
    "                    dict_key = temp\n",
    "                    # print(dict_key)\n",
    "\n",
    "                    if class_names[preds[0]] == \"Has_DKD\":\n",
    "                        # heat = np.full(shape=(512, 512,3),fill_value=(0,0,255)).astype(np.uint32)\n",
    "                        # heat = Image.fromarray((heat * 255).astype(np.uint8)) \n",
    "                        heat = Image.new('RGB',(int(512/SCALE_FACTOR),int(512/SCALE_FACTOR)),(255,0,0))\n",
    "                    elif class_names[preds[0]] == \"Healthy\":\n",
    "                        # heat = np.full(shape=(512, 512,3),fill_value=(0,255,0)).astype(np.uint32)\n",
    "                        # heat = Image.fromarray((heat * 255).astype(np.uint8))\n",
    "                        heat = Image.new('RGB',(int(512/SCALE_FACTOR),int(512/SCALE_FACTOR)),(0,255,0)) \n",
    "                    elif class_names[preds[0]] == \"H-CKD\":\n",
    "                        # heat = np.full(shape=(512, 512,3),fill_value=(255,0,0)).astype(np.uint32)\n",
    "                        # heat = Image.fromarray((heat * 255).astype(np.uint8))\n",
    "                        heat = Image.new('RGB',(int(512/SCALE_FACTOR),int(512/SCALE_FACTOR)),(0,0,255)) \n",
    "                    # heat = Image.new('RGB',(512,512),(255,0,0))\n",
    "                    # heat.save(\"just_img.png\")\n",
    "                    if (tsv_coords[tsv_coords[\"Tile\"] == (dict_key)]['Keep'].iloc[0] == 1):\n",
    "                        \n",
    "                        x = int((tsv_coords[tsv_coords[\"Tile\"] == (dict_key)]['Column'].iloc[0])*int(512)/SCALE_FACTOR)\n",
    "                        y = int((tsv_coords[tsv_coords[\"Tile\"] == (dict_key)]['Row'].iloc[0])*int(512)/SCALE_FACTOR)\n",
    "                        print((tsv_coords[tsv_coords[\"Tile\"] == (dict_key)]['Keep'].iloc[0])) \n",
    "                        heat_map.paste(im=heat, box=(x,y))\n",
    "\n",
    "                    # print(x,y)\n",
    "            except IndexError:\n",
    "                pass\n",
    "            try:\n",
    "                \n",
    "                if filenames[1]:\n",
    "                    path_sum+=1\n",
    "                    # print(\"path 1\", filenames[1])\n",
    "                    # print(\"prediction: \", class_names[preds[1]])\n",
    "                    temp = filenames[1].split(\"/\")[-1].split(\".\")[0]\n",
    "                    dict_key = temp\n",
    "                    # print(dict_key)                \n",
    "\n",
    "                    if class_names[preds[1]] == \"Has_DKD\":\n",
    "                        #  heat = np.full(shape=(512, 512,3),fill_value=(0,0,255)).astype(np.uint32)\n",
    "                        #  heat = Image.fromarray((heat * 255).astype(np.uint8))\n",
    "                         heat = Image.new('RGB',(int(512/SCALE_FACTOR),int(512/SCALE_FACTOR)),(255,0,0)) \n",
    "                    elif class_names[preds[1]] == \"Healthy\":\n",
    "                        #  heat = np.full(shape=(512, 512,3),fill_value=(0,255,0)).astype(np.uint32)\n",
    "                        #  heat = Image.fromarray((heat * 255).astype(np.uint8))\n",
    "                         heat = Image.new('RGB',(int(512/SCALE_FACTOR),int(512/SCALE_FACTOR)),(0,255,0)) \n",
    "                    elif class_names[preds[1]] == \"H-CKD\":\n",
    "                        #  heat = np.full(shape=(512, 512,3),fill_value=(255,0,0)).astype(np.uint32)\n",
    "                        #  heat = Image.fromarray((heat * 255).astype(np.uint8))\n",
    "                         heat = Image.new('RGB',(int(512/SCALE_FACTOR),int(512/SCALE_FACTOR)),(0,0,255)) \n",
    "                    \n",
    "                    if (tsv_coords[tsv_coords[\"Tile\"] == (dict_key)]['Keep'].iloc[0] == 1):\n",
    "                        \n",
    "                        x = int((tsv_coords[tsv_coords[\"Tile\"] == (dict_key)]['Column'].iloc[0])*int(512)/SCALE_FACTOR)\n",
    "                        y = int((tsv_coords[tsv_coords[\"Tile\"] == (dict_key)]['Row'].iloc[0])*int(512)/SCALE_FACTOR)\n",
    "                        print((tsv_coords[tsv_coords[\"Tile\"] == (dict_key)]['Keep'].iloc[0])) \n",
    "                        heat_map.paste(im=heat, box=(x,y))\n",
    "                    \n",
    "    #             print(filenames[2])\n",
    "            except IndexError:\n",
    "                pass\n",
    "            try:\n",
    "                if filenames[2]:\n",
    "                    path_sum+=1\n",
    "                    # print(\"path 2\", filenames[2])\n",
    "                    # print(\"prediction: \", class_names[preds[2]])\n",
    "                    temp = filenames[2].split(\"/\")[-1].split(\".\")[0]\n",
    "                    dict_key = temp\n",
    "#                     dict_key = temp[0]+\"_\"+temp[1]+\"_\"+temp[2]+\"_\"+temp[3]\n",
    "                    # print(dict_key)                    \n",
    "\n",
    "                    if class_names[preds[2]] == \"Has_DKD\":\n",
    "                        # heat = np.full(shape=(512, 512,3),fill_value=(0,0,255)).astype(np.uint32)\n",
    "                        # heat = Image.fromarray((heat * 255).astype(np.uint8))\n",
    "                        heat = Image.new('RGB',(int(512/SCALE_FACTOR),int(512/SCALE_FACTOR)),(255,0,0)) \n",
    "                    elif class_names[preds[2]] == \"Healthy\":\n",
    "                        # heat = np.full(shape=(512, 512,3),fill_value=(0,255,0)).astype(np.uint32)\n",
    "                        # heat = Image.fromarray((heat * 255).astype(np.uint8))\n",
    "                        heat = Image.new('RGB',(int(512/SCALE_FACTOR),int(512/SCALE_FACTOR)),(0,255,0)) \n",
    "                    elif class_names[preds[2]] == \"H-CKD\":\n",
    "                        # heat = np.full(shape=(512, 512,3),fill_value=(255,0,0)).astype(np.uint32)\n",
    "                        # heat = Image.fromarray((heat * 255).astype(np.uint8))\n",
    "                        heat = Image.new('RGB',(int(512/SCALE_FACTOR),int(512/SCALE_FACTOR)),(0,0,255)) \n",
    "                    \n",
    "                    if (tsv_coords[tsv_coords[\"Tile\"] == (dict_key)]['Keep'].iloc[0] == 1):\n",
    "                        \n",
    "                        x = int((tsv_coords[tsv_coords[\"Tile\"] == (dict_key)]['Column'].iloc[0])*int(512)/SCALE_FACTOR)\n",
    "                        y = int((tsv_coords[tsv_coords[\"Tile\"] == (dict_key)]['Row'].iloc[0])*int(512)/SCALE_FACTOR)\n",
    "                        print((tsv_coords[tsv_coords[\"Tile\"] == (dict_key)]['Keep'].iloc[0])) \n",
    "                        heat_map.paste(im=heat, box=(x,y))  \n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                if filenames[3]:\n",
    "                    path_sum+=1\n",
    "                    # print(\"path 3\", filenames[3])\n",
    "                    # print(\"prediction: \", class_names[preds[3]])\n",
    "                    temp = filenames[3].split(\"/\")[-1].split(\".\")[0]\n",
    "                    dict_key = temp\n",
    "#                     dict_key = temp[0]+\"_\"+temp[1]+\"_\"+temp[2]+\"_\"+temp[3]\n",
    "                    # print(dict_key)\n",
    "\n",
    "                    if class_names[preds[3]] == \"Has_DKD\":\n",
    "                        # heat = np.full(shape=(512, 512, 3),fill_value=(0,0,255)).astype(np.uint32)\n",
    "                        # heat = Image.fromarray((heat * 255).astype(np.uint8))\n",
    "                        heat = Image.new('RGB',(int(512/SCALE_FACTOR),int(512/SCALE_FACTOR)),(255,0,0))\n",
    "                    elif class_names[preds[3]] == \"Healthy\":\n",
    "                        # heat = np.full(shape=(512, 512, 3),fill_value=(0,255,0)).astype(np.uint32)\n",
    "                        # heat = Image.fromarray((heat * 255).astype(np.uint8))\n",
    "                        heat = Image.new('RGB',(int(512/SCALE_FACTOR),int(512/SCALE_FACTOR)),(0,255,0))\n",
    "                    elif class_names[preds[3]] == \"H-CKD\":\n",
    "                        # heat = np.full(shape=(512, 512, 3),fill_value=(255,0,0)).astype(np.uint32)\n",
    "                        # heat = Image.fromarray((heat * 255).astype(np.uint8))\n",
    "                        heat = Image.new('RGB',(int(512/SCALE_FACTOR),int(512/SCALE_FACTOR)),(0,0,255))\n",
    "                    \n",
    "                    if (tsv_coords[tsv_coords[\"Tile\"] == (dict_key)]['Keep'].iloc[0] == 1):\n",
    "                        \n",
    "                        x = int((tsv_coords[tsv_coords[\"Tile\"] == (dict_key)]['Column'].iloc[0])*int(512)/SCALE_FACTOR)\n",
    "                        y = int((tsv_coords[tsv_coords[\"Tile\"] == (dict_key)]['Row'].iloc[0])*int(512)/SCALE_FACTOR)\n",
    "                        print((tsv_coords[tsv_coords[\"Tile\"] == (dict_key)]['Keep'].iloc[0])) \n",
    "                        heat_map.paste(im=heat, box=(x,y))\n",
    "                    \n",
    "            except IndexError:\n",
    "                pass\n",
    "        heat_map.save('mask.png')\n",
    "        downsampled_img = Image.blend(ds_img, heat_map, 0.4)\n",
    "        downsampled_img.save(\"heatmap.png\")\n",
    "        print(path_sum)\n",
    "        \n",
    "        \n",
    "test_list = df_labels[\"filenames\"].tolist()\n",
    "print(test_list[0])\n",
    "wsi_label = df_labels[df_labels[\"filenames\"] == (test_list[0])]['labels'].iloc[0]\n",
    "print(wsi_label)\n",
    "model = torch.load(PATH)\n",
    "model.eval()\n",
    "heatmap_generation(model,test_list[0],wsi_label) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
